{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Model for Toxic Comments Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this notebook, our task is to classify the comments for toxicity. The comments have been aggregated from wikipedia talkpage. The dataset for this NLP tutprial comes from a recent Kaggle competition sponsered by Google. The training data consists of over 150k comments labeled by human annotators. The test data is little over 150k where we need to predict the label.  \n",
    "\n",
    "One of the challenges is the imbalance of positivie (toxic labeled comments) and negative class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always, we first import libraries from Numpy, Keras, Pandas, and sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, log_loss\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, SpatialDropout1D, concatenate, CuDNNGRU\n",
    "from keras.layers import GRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.layers import Dropout, BatchNormalization\n",
    "from keras.callbacks import Callback\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load files before we move any further. We read training and test data files provided in csv format using Pandas dataframes. \n",
    "\n",
    "We also need to load pre-trained word vector embedding. Later we will describe why and how we use the word embedding in our recurrent neural net model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')\n",
    "submission = pd.read_csv('data/sample_submission.csv')\n",
    "\n",
    "EMBEDDING_FILE_glove = 'vectors/glove.840B.300d.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some data re-arranging here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train[\"comment_text\"].fillna(\"fillna\").values\n",
    "y_train = train[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].values\n",
    "X_test = test[\"comment_text\"].fillna(\"fillna\").values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model Parameters\n",
    "\n",
    "It's time to set some of the most important model parameters. We will need to tune these parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 200000\n",
    "maxlen = 300\n",
    "embed_size = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Tokenizer\n",
    "After setting-up the basics, now let's dive into the actual process of deep learning model development.\n",
    "First of all, we will tokenize the texts. What it means is that we will convert texts into matrices of number. Why we do this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(X_train) + list(X_test))\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "x_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(X_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Word Vector Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.1 Glove Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = dict()\n",
    "f = open(EMBEDDING_FILE_glove)\n",
    "\n",
    "for line in f:\n",
    "    # Note: use split(' ') instead of split() if you get an error.\n",
    "    values = line.split(' ')\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "all_embs = np.stack(embeddings_index.values())\n",
    "emb_mean, emb_std = all_embs.mean(), all_embs.std()\n",
    "# Create the weight matrix\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, (len(word_index)-1))\n",
    "\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. RNN Model\n",
    "\n",
    "Now, we have reached a point where we can develop our recurrent neural net model using Keras.\n",
    "\n",
    "\n",
    "Here are the steps:\n",
    "\n",
    "1. We create embedding layer with the embeddign matrix that we created before.\n",
    "2. Right after embedding, we define a dropout layer.\n",
    "3. We use CuDNNGRU as our recurrent layer (If not using GPU, replace CuDNNGRU with GRU).\n",
    "4. Next, we apply average and max pooling and concatenate them.\n",
    "4. Finally, we pass the output from concatenation into a dense layer with 6 outputs, one each for a class label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_6 (Embedding)         (None, 300, 300)     60000000    input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_6 (SpatialDro (None, 300, 300)     0           embedding_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_6 (Bidirectional) (None, 300, 128)     140544      spatial_dropout1d_6[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_6 (Glo (None, 128)          0           bidirectional_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_6 (GlobalM (None, 128)          0           bidirectional_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 256)          0           global_average_pooling1d_6[0][0] \n",
      "                                                                 global_max_pooling1d_6[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 6)            1542        concatenate_6[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 60,142,086\n",
      "Trainable params: 60,142,086\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def get_model():\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n",
    "    x = SpatialDropout1D(0.5)(x)\n",
    "    x = Bidirectional(CuDNNGRU(64, return_sequences=True))(x)\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    conc = concatenate([avg_pool, max_pool])\n",
    "    outp = Dense(6, activation=\"sigmoid\")(conc)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "model = get_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. ROC AUC Metric\n",
    "\n",
    "Currently, Keras doest not have ROC AUC metric and so we will write some code to monitor ROC AUC for validation set loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def _train_model(model, batch_size, train_x, train_y, val_x, val_y):\n",
    "    best_loss = -1\n",
    "    best_weights = None\n",
    "    best_epoch = 0\n",
    "\n",
    "    current_epoch = 0\n",
    "\n",
    "    while True:\n",
    "        model.fit(train_x, train_y, batch_size=batch_size, epochs=1)\n",
    "        y_pred = model.predict(val_x, batch_size=batch_size)\n",
    "\n",
    "        total_loss = 0\n",
    "        for j in range(6):\n",
    "            loss = roc_auc_score(val_y[:, j], y_pred[:, j])\n",
    "            total_loss += loss\n",
    "\n",
    "        total_loss /= 6.\n",
    "\n",
    "        print(\"Epoch {0} loss {1} best_loss {2}\".format(current_epoch, total_loss, best_loss))\n",
    "\n",
    "        current_epoch += 1\n",
    "        if total_loss > best_loss or best_loss == -1:\n",
    "            best_loss = total_loss\n",
    "            best_weights = model.get_weights()\n",
    "            best_epoch = current_epoch\n",
    "        else:\n",
    "            if current_epoch - best_epoch == 1:\n",
    "                break\n",
    "\n",
    "    model.set_weights(best_weights)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_folds(X, y, fold_count, batch_size, get_model_func):\n",
    "    fold_size = len(X) // fold_count\n",
    "    models = []\n",
    "    for fold_id in range(0, fold_count):\n",
    "        fold_start = fold_size * fold_id\n",
    "        fold_end = fold_start + fold_size\n",
    "\n",
    "        if fold_id == fold_size - 1:\n",
    "            fold_end = len(X)\n",
    "\n",
    "        train_x = np.concatenate([X[:fold_start], X[fold_end:]])\n",
    "        train_y = np.concatenate([y[:fold_start], y[fold_end:]])\n",
    "\n",
    "        val_x = X[fold_start:fold_end]\n",
    "        val_y = y[fold_start:fold_end]\n",
    "\n",
    "        model = _train_model(get_model_func(), batch_size, train_x, train_y, val_x, val_y)\n",
    "        models.append(model)\n",
    "\n",
    "    return models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Train Model\n",
    "\n",
    "Now is the time to actually train the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_folds = 5\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "127657/127657 [==============================] - 369s 3ms/step - loss: 0.0535 - acc: 0.9807\n",
      "Epoch 0 loss 0.9875802368268326 best_loss -1\n",
      "Epoch 1/1\n",
      "127657/127657 [==============================] - 369s 3ms/step - loss: 0.0391 - acc: 0.9846\n",
      "Epoch 1 loss 0.9894354568612592 best_loss 0.9875802368268326\n",
      "Epoch 1/1\n",
      "127657/127657 [==============================] - 368s 3ms/step - loss: 0.0344 - acc: 0.9863\n",
      "Epoch 2 loss 0.9899473566953375 best_loss 0.9894354568612592\n",
      "Epoch 1/1\n",
      "127657/127657 [==============================] - 368s 3ms/step - loss: 0.0306 - acc: 0.9879\n",
      "Epoch 3 loss 0.9885625357296844 best_loss 0.9899473566953375\n",
      "Epoch 1/1\n",
      "127657/127657 [==============================] - 370s 3ms/step - loss: 0.0530 - acc: 0.9809\n",
      "Epoch 0 loss 0.9860735579645251 best_loss -1\n",
      "Epoch 1/1\n",
      "127657/127657 [==============================] - 369s 3ms/step - loss: 0.0391 - acc: 0.9845\n",
      "Epoch 1 loss 0.9886888878369549 best_loss 0.9860735579645251\n",
      "Epoch 1/1\n",
      "127657/127657 [==============================] - 369s 3ms/step - loss: 0.0346 - acc: 0.9861\n",
      "Epoch 2 loss 0.9885022739827657 best_loss 0.9886888878369549\n",
      "Epoch 1/1\n",
      "127657/127657 [==============================] - 370s 3ms/step - loss: 0.0534 - acc: 0.9806\n",
      "Epoch 0 loss 0.9865644894489897 best_loss -1\n",
      "Epoch 1/1\n",
      "127657/127657 [==============================] - 369s 3ms/step - loss: 0.0392 - acc: 0.9845\n",
      "Epoch 1 loss 0.9877616814238852 best_loss 0.9865644894489897\n",
      "Epoch 1/1\n",
      "127657/127657 [==============================] - 369s 3ms/step - loss: 0.0346 - acc: 0.9862\n",
      "Epoch 2 loss 0.987168218123233 best_loss 0.9877616814238852\n",
      "Epoch 1/1\n",
      "127657/127657 [==============================] - 371s 3ms/step - loss: 0.0534 - acc: 0.9807\n",
      "Epoch 0 loss 0.9854899909067051 best_loss -1\n",
      "Epoch 1/1\n",
      "127657/127657 [==============================] - 369s 3ms/step - loss: 0.0391 - acc: 0.9846\n",
      "Epoch 1 loss 0.9883896936574522 best_loss 0.9854899909067051\n",
      "Epoch 1/1\n",
      "127657/127657 [==============================] - 369s 3ms/step - loss: 0.0347 - acc: 0.9862\n",
      "Epoch 2 loss 0.9882708127540151 best_loss 0.9883896936574522\n",
      "Epoch 1/1\n",
      "127657/127657 [==============================] - 370s 3ms/step - loss: 0.0533 - acc: 0.9809\n",
      "Epoch 0 loss 0.9862962253492219 best_loss -1\n",
      "Epoch 1/1\n",
      "127657/127657 [==============================] - 369s 3ms/step - loss: 0.0392 - acc: 0.9846\n",
      "Epoch 1 loss 0.9877997708214594 best_loss 0.9862962253492219\n",
      "Epoch 1/1\n",
      "127657/127657 [==============================] - 369s 3ms/step - loss: 0.0346 - acc: 0.9860\n",
      "Epoch 2 loss 0.9879537726937967 best_loss 0.9877997708214594\n",
      "Epoch 1/1\n",
      "127657/127657 [==============================] - 369s 3ms/step - loss: 0.0306 - acc: 0.9876\n",
      "Epoch 3 loss 0.9877041425225408 best_loss 0.9879537726937967\n"
     ]
    }
   ],
   "source": [
    "models = train_folds(x_train, y_train, nr_folds, batch_size, get_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Create Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we create predictions for test and create submission file. The predictions are ensembled from each fold using geometric mean and numpy array predictions files for each fold are also saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "import os\n",
    "if not os.path.exists(\"nlp_tut\"):\n",
    "        os.mkdir(\"nlp_tut\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting results...\n",
      "Finished creating submission!\n"
     ]
    }
   ],
   "source": [
    "print(\"Predicting results...\")\n",
    "test_predicts_list = []\n",
    "for fold_id, model in enumerate(models):\n",
    "    model_path = os.path.join(\"nlp_tut\", \"model{0}_weights.npy\".format(fold_id))\n",
    "    np.save(model_path, model.get_weights())\n",
    "\n",
    "    test_predicts_path = os.path.join(\"nlp_tut\", \"test_predicts{0}.npy\".format(fold_id))\n",
    "    test_predicts = model.predict(x_test, batch_size=256)\n",
    "    test_predicts_list.append(test_predicts)\n",
    "    np.save(test_predicts_path, test_predicts)\n",
    "\n",
    "test_predicts = np.ones(test_predicts_list[0].shape)\n",
    "for fold_predict in test_predicts_list:\n",
    "    test_predicts *= fold_predict\n",
    "\n",
    "test_predicts **= (1. / len(test_predicts_list))\n",
    "\n",
    "test_ids = test[\"id\"].values\n",
    "test_ids = test_ids.reshape((len(test_ids), 1))\n",
    "\n",
    "test_predicts = pd.DataFrame(data=test_predicts, columns=CLASSES)\n",
    "test_predicts[\"id\"] = test_ids\n",
    "test_predicts = test_predicts[[\"id\"] + CLASSES]\n",
    "submit_path = os.path.join(\"nlp_tut\", \"submit\")\n",
    "test_predicts.to_csv(submit_path, index=False)\n",
    "\n",
    "print(\"Finished creating submission!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Acknowledgements\n",
    "\n",
    "This notebook is based on my learning from a number of wonderful people sharing their insights, models, and codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
